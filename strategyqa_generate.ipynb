{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def generate(prompt,max_tokens=250, temperature=0):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "\n",
    "    return response[\"choices\"][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "import json\n",
    "\n",
    "searcher = SimpleSearcher('./data/strategyqa_index_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = json.load(open(\"/datadrive/multidocqa/data/strategyqa_reranked.json\",'r'))\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import argparse\n",
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    return re.sub(regex, ' ', text)\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "  def remove_accents(input_str):\n",
    "      nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "      only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "      return only_ascii.decode(\"utf-8\")\n",
    "\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(remove_accents(s)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 20:25:25.036525: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-14 20:25:25.036567: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('sentence-transformers/msmarco-bert-base-dot-v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_train = json.load(open(\"data/explained_iirc_boolean.json\",'r'))\n",
    "explained_train = [a for a in explained_train if a['answer'][\"type\"]==\"binary\" and \"explanation\" in a]\n",
    "train_embeddings = model.encode([a['question'] for a in explained_train])\n",
    "len(explained_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_train = json.load(open(\"data/qasper_explanation.json\",\"r\"))\n",
    "explained_train = [a for a in explained_train if a['answer'].lower() in [\"no\",\"yes\"]]\n",
    "train_embeddings = model.encode([a['question'] for a in explained_train])\n",
    "len(explained_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "def generate_answer(documents, question,kshot=2):\n",
    "    prompt=\"For each example, use the documents to create an \\\"Answer\\\" and an \\\"Explanation\\\" to the \\\"Question\\\". Just answer yes or no.\\n\\nExample 1:\\n\\n[Document 1]: Title: San Tropez (song). Content: \\\"San Tropez\\\" is the fourth track from the album Meddle by the band Pink Floyd. This song was one of several to be considered for the band's \\\"best of\\\" album, Echoes: The Best of Pink Floyd.\\n    \\n[Document 2]: Title: French Riviera. Content: The French Riviera (known in French as the Côte d'Azur [kot daˈzyʁ]; Occitan: Còsta d'Azur [ˈkɔstɔ daˈzyɾ]; literal translation \\\"Azure Coast\\\") is the Mediterranean coastline of the southeast corner of France. There is no official boundary, but it is usually considered to extend from Cassis, Toulon or Saint-Tropez on the west to Menton at the France–Italy border in the east, where the Italian Riviera joins. The coast is entirely within the Provence-Alpes-Côte d'Azur (Région Sud) region of France. The Principality of Monaco is a semi-enclave within the region, surrounded on three sides by France and fronting the Mediterranean.\\n    \\n[Document 3]: Title: Moon Jae-in. Content: Moon also promised transparency in his presidency, moving the presidential residence from the palatial and isolated Blue House to an existing government complex in downtown Seoul.\\n\\n[Document 4]: Title: Saint-Tropez. Content: Saint-Tropez (US: /ˌsæn troʊˈpeɪ/ SAN-troh-PAY, French: [sɛ̃ tʁɔpe]; Occitan: Sant-Tropetz , pronounced [san(t) tʀuˈpes]) is a town on the French Riviera, 68 kilometres (42 miles) west of Nice and 100 kilometres (62 miles) east of Marseille in the Var department of the Provence-Alpes-Côte d'Azur region of Occitania, Southern France.\\n\\nQuestion: Did Pink Floyd have a song about the French Riviera?\\n\\nExplanation: According to [Document 1], \\\"San Tropez\\\" is a song by Pink Floyd about the French Riviera. This is further supported by [Document 4], which states that Saint-Tropez is a town on the French Riviera. Therefore, the answer is yes\\n\\nAnswer: yes.\\n\\n\"\n",
    "\n",
    "    for i, document in enumerate(documents):\n",
    "        # prompt += \"[Document {0}]: Title: {2}. Content: {1}\\n\\n\".format(i+1, document['contents'], document['title'])\n",
    "        prompt += \"[Document {0}]: {1}\\n\\n\".format(i+1, document['contents'])\n",
    "\n",
    "    prompt+= \"Question: {0}\\n\\nExplanation: According\".format(question)\n",
    "\n",
    "    tokens = tokenizer.tokenize(prompt)\n",
    "    if len(tokens)+266 > 4000:\n",
    "        return generate_answer(documents[:-1], question)\n",
    "\n",
    "    to_return = {\n",
    "        \"prompt\": prompt,\n",
    "        \"documents\": documents,\n",
    "        \"retry\": False,\n",
    "        \"tokens\": len(tokens)\n",
    "    }\n",
    "\n",
    "    res = generate(prompt)\n",
    "\n",
    "    to_return['res'] = res\n",
    "\n",
    "    if \"answer:\" not in res.lower():\n",
    "        to_return['retry'] = True\n",
    "        prompt = prompt+\" According\"+res+\"\\nAnswer:\"\n",
    "        to_return['new_prompt'] = prompt\n",
    "        tokens = tokenizer.tokenize(prompt)\n",
    "        to_return['tokens'] = len(tokens)\n",
    "        res = \" According\"+res+\"\\nAnswer:\"+ generate(prompt,max_tokens=10)\n",
    "        to_return['res']= res\n",
    "    \n",
    "    explanation = res.lower().split(\"answer:\")[0]\n",
    "    to_return['explanation'] = explanation\n",
    "    answer = res.lower().split(\"answer:\")[1]\n",
    "    to_return['answer'] = answer\n",
    "    to_return['evidence'] = list(set([d['m_id'] for d in documents]))\n",
    "\n",
    "    return to_return"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
